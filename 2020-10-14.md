# Daily Context & Goals

I've been feeling pretty confused as to which tasks to work on. Unit testing
has proven to be both highly detailed, and incremental, which makes it feel
like I'm not working toward a broader task. This is similarly reflected in the
way I'm talking about "Unit Testing" instead of WHICH unit tests I want to
complete.

Today I'm going to start by writing down in detail what unit tests we've
already completed, which are roughly:

* Generate from model prior (checks model is not crashing)
* single timestep, single object, checking the logpdf is a constant
  multiplicative factor off from the gaussianVMF.
* Single timestep, two sliding objects with fixed observed slack. Varying the
  latents over a range including the subset [0, observed_slack_offset], and
  visualizing the posterior. Assert the mode of the posterior is in-between
* 

and which unit tests we're thinking about implementing:

* Placeholder
* Placeholder
* Placeholder


Challenges we've run into while unit testing so far:

* Making sure the trace is constrained properly, and that we're not
  accidentally sampling certain latents or observations without realizing it.
    * Solution: Write a `addr_space_is_covered(trace, constrained_vars, sampled_vars)`
      function that checks that the address space of trace is the union of
      constrained_vars and sampled_vars.
* Specifying the unit test simultaneously rigorously and clearly, is challenging.
    * Solution: Draw a picture for each unit test that describes what exactly the
      observation and latents, and what we're varying by showing like 3 frames of
      each test. Upload to Google Drive, add static link in unit test docstring.
* Unit testing is incredibly incremental; it's hard to write down a spec for
  all unit tests we want to do without building up intuition first through
  previous unit tests.
    * Solution: Don't expect to write down all the unit tests right now. Focus
      on clarifying and tracking the ones already completed, and the next set
      of tests we're working on right now.


Writing this context out, it's clear I need to focus on _simplifying_ and
_tracking_ what work has been completed for the unit tests, by developing a
clearer set of deliverables for each individual test. Each test should have:

* Careful consideration paid to variable naming conventions within each test.
  Liberal usage of code comments, and simplifying common patterns with unit
  testing infrastructure.
* A clear, descriptive docstring that is as concise as possible without losing
  any detail.
* A figure representing what the unit test is actually exploring.
* A static reference from the google doc, to the line of code containing the
  unit test.

Particularly, I should be focused on making the unit tests interpretable to an
external audience (Marco/Vikash), to allow for feedback on existing and planned
tests. Today will be oriented around that specific goal, which will set me up
tomorrow to more confidently set out a higher-level goal for each type of unit
test (eg. single/multi-frame model/inference correctness).


# Prospective Tasks

* [ ] Write down basic docstring for each unit test. Copy static link to google
      doc, with short headline describing the high-level summary of the test's
      objectives
* [ ] Implement `addr_space_is_covered`
* [ ] Review existing unit tests, refactor with infrastructure and rename variables
      to be more precise.
    * [ ] Fix infrastructure if unable to recovered original test results.
* [ ] Draw picture for each unit test. Upload to Google Drive, and add static
      link to docstring.


# Daily Reflection
